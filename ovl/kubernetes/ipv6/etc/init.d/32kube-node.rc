#! /bin/sh

export PATH="/bin:/sbin:/usr/bin:/usr/sbin"

die() {
        echo "$@"
        exit 1
}

hostname | grep -Eq 'vm-[0-9]+$' || die "Invalid hostname [$(hostname)]"
i=$(hostname | cut -d- -f2 | sed -re 's,^0+,,')
test $i -le 200 || exit 0

. /etc/profile
test -n "$DOMAIN" || DOMAIN=xcluster

# Add route to the private Docker registry
ip -6 route add fd00:2008::/64 via 2000::250

# Cri-o *requires* a default route. Any route seems to do nicely, even
# an ipv4 route on a ipv6 cluster.
ip -6 route add default \
	nexthop via 1000::1:192.168.1.201 nexthop via 1000::1:192.168.1.202
crio -log-level debug > /var/log/crio.log 2>&1 &
sleep 0.2

# See;
# https://stackoverflow.com/questions/46726216/kubelet-fails-to-get-cgroup-stats-for-docker-and-kubelet-services

run_kubelet() {
	kubelet --container-runtime=remote --address=:: --node-ip=1000::1:192.168.1.$i \
		--container-runtime-endpoint=unix:///var/run/crio/crio.sock \
		--image-service-endpoint=unix:///var/run/crio/crio.sock \
		--register-node=true --kubeconfig $KUBECONFIG \
		--network-plugin=cni \
		--cluster-dns=1000::1:192.168.1.$i --cluster-domain=$DOMAIN \
		--runtime-cgroups=/systemd/system.slice \
		--kubelet-cgroups=/systemd/system.slice >> /var/log/kubelet.log 2>&1
}

run_kube_proxy() {
	kube-proxy --config /etc/kubernetes/kube-proxy.config \
		>> /var/log/kube-proxy.log 2>&1
}

monitor() {
	while true; do
		logger -t K8s-monitor "STARTING: $1"
		$1
		logger -t K8s-monitor "DIED: $1"
		sleep 2
	done
}

(monitor run_kubelet) > /dev/null 2>&1 &
(monitor run_kube_proxy) > /dev/null 2>&1 &

# For --proxy-mode=iptables you must add;
#ip -6 ro add local fd00:4000::/112 dev lo
