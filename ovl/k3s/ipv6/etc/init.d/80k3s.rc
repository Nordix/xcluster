#! /bin/sh
die() {
	echo "$@"
	exit 1
}

hostname | grep -Eq 'vm-[0-9]+$' || die "Invalid hostname [$(hostname)]"
i=$(hostname | cut -d- -f2 | sed -re 's,^0+,,')

test $i -le 200 || exit 0

if test -x /bin/kube-proxy; then
	echo "FAIL: K8s is installed, k3s is disabled!!"
	echo "echo 'FAIL: K8s is installed, k3s is disabled!!'" >> /root/.profile
	return 1
fi

# Generate a /etc/hosts
cat > /etc/hosts <<EOF
::1                     localhost.localdomain localhost
127.0.0.1               localhost.localdomain localhost
EOF
for x in $(seq 1 24); do
	printf "1000::1:192.168.1.$x vm-%03d\\n" $x >> /etc/hosts
done

# Work around for bad install of dropbear
test -r /usr/bin/dbclient || ln /bin/dbclient /usr/bin/dbclient

mount -t cgroup cgroup /sys/fs/cgroup

# This is required for; https://github.com/rancher/k3s/issues/259
#ip ro replace default via 192.168.1.201
#ip -6 ro replace default via 1000::1:192.168.1.201

export K3S_CLUSTER_SECRET=SECRET
echo "alias kubectl='k3s kubectl'" >> /etc/profile

if test $i -eq 1; then
	echo "export KUBECONFIG=/etc/kubernetes/kubeconfig" >> /etc/profile
	# Start the server only on vm-001
	(exec k3s server --no-flannel --no-deploy coredns \
		--no-deploy servicelb --no-deploy traefik --disable-agent \
		--write-kubeconfig /etc/kubernetes/kubeconfig --tls-san 192.168.0.1 \
		--cluster-cidr 1000::2:11.0.0.0/112 --service-cidr fd00:4000::/112 \
		--node-ip 1000::1:192.168.1.$i \
		> /var/log/k3s-server.log 2>&1) &
	exit 0
fi

echo "export KUBECONFIG=/var/lib/rancher/k3s/agent/kubeconfig.yaml" >> /etc/profile

# Start agent only on all nodes except vm-001
(exec k3s agent --server https://192.168.1.1:6443 \
	--no-flannel --containerd-config-template /etc/containerd.conf \
	--node-ip 1000::1:192.168.1.$i > /var/log/k3s-agent.log 2>&1) &

